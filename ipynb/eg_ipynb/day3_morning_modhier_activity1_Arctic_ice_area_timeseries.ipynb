{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Northern Hemisphere sea ice area in the CESM model hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Science goals\n",
    "- Examine differences in the seasonal cycle and time series of Arctic sea ice area in simulations with different compsets <br>\n",
    "- Understand the effects of prescribing climatological SST/sea ice (F compset) <br>\n",
    "- Hypothesize reasons for time scale differences in B and E compset cases <br>\n",
    "- Brainstorm effective strategies (statistical or otherwise) for comparing time series of two simulations from different compsets\n",
    "\n",
    "\n",
    "### Technical goals\n",
    "- Learn how to examine a single netCDF variable as an xarray DataArray object <br>\n",
    "- Make minor changes to simple Python plotting routines <br>\n",
    "- Gain a little familiarity with time series analysis in Python <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, take a moment to remind yourself which letters correspond to which compsets. <br>\n",
    "\n",
    "In particular, what are the B, E, F, and G compsets? <br>\n",
    "\n",
    "Look 'em up here: http://www.cesm.ucar.edu/models/cesm2/cesm/compsets.html\n",
    "\n",
    "Remember: To execute a cell of Python code, type SHIFT+ENTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in packages and simple xarray manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minimal packages needed for this activity\n",
    "import xarray as xr                          #for slick netCDF operations\n",
    "import numpy as np                           #for standard numeric manipulation\n",
    "import matplotlib.pyplot as plt              #standard plotting library based upon Matlab functions\n",
    "import pickle                                #library for saving and loading Python variables into \"pickle\" format\n",
    "from scipy import signal                     #loading frequency analysis part of scientific computing library \"scipy\"\n",
    "from scipy import stats                      #loading stats part of scientific computing library \"scipy\"\n",
    "from matplotlib.gridspec import GridSpec     #for multipanel plotting\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#a couple of  constants\n",
    "m2_to_millionkm2=1e6*1e6\n",
    "d2r=np.pi/180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the limited memory on the cheyenne compute nodes which we are using, the time series and monthly climatologies have already been processed from the original large model output and saved in a Python data format called a \"pickle.\"  These pickles contain an object called a DataArray from the library xarray. DataArrays are a representation of a single netCDF variable and all of that variable's associated dimensions/coordinates, attributes, and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'aice_nh_total_bcase.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6df9eb6fdffe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#loading in pre-processed time series of NH ice area\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0maice_nh_total_bcase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"aice_nh_total_bcase.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0maice_nh_total_ecase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"aice_nh_total_ecase.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maice_nh_total_fcase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"aice_nh_total_fcase.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'aice_nh_total_bcase.p'"
     ]
    }
   ],
   "source": [
    "#loading in pre-processed time series of NH ice area\n",
    "\n",
    "aice_nh_total_bcase=pickle.load(open( \"aice_nh_total_bcase.p\", \"rb\" ))\n",
    "aice_nh_total_ecase=pickle.load(open( \"aice_nh_total_ecase.p\", \"rb\" ))\n",
    "aice_nh_total_fcase=pickle.load(open( \"aice_nh_total_fcase.p\", \"rb\" ))\n",
    "aice_nh_total_gcase=pickle.load(open( \"aice_nh_total_gcase.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To examine what's in these DataArrays, try printing them and their output. <br> <br>\n",
    "Sometimes other Python libraries choke on DataArrays, so you often need to pull the variable out of the DataArray object into a numpy array. To do this, add the attribute \".values\" to the end of the DataArray object. Numpy arrays are the standard Python object for scientific computation. More advanced data analysis techniques are found in the scipy library, and scipy functions/methods work seamlessly with numpy arrays.\n",
    " \n",
    "*(Side note: methods and functions are slightly different things in Python, though they look similar and can be used in very similar ways. Functions are independent blocks of code that do not change the state of an object, have arguments, and optionally return something (e.g., library.function(ARGUMENTS)).  Methods are dependent upon Python objects (are part of the object's \"class\"), must be called with these objects, and can change the state of the object (e.g., object.method(PARAMETERS) or object.method()). Of course, it isn't this simple... but this quick definition should help you distinguish the methods and functions ~90% of the time.*\n",
    "\n",
    "*(Side side note: Here's how to tell apart an object's attributes from functions/methods. Attributes have no parentheses (e.g., object.attribute), while methods/functions do (e.g., object.method(), library.function(ARG)) )*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('This is the full DataArray for the B-compset total ice area:\\n')\n",
    "print(aice_nh_total_bcase,'\\n\\n') \n",
    "#putting \\n into a string gives you a new line - this is just a formatting choice here to make things print prettier.\n",
    "\n",
    "print('These are the coordinates for this DataArray:\\n')\n",
    "print(aice_nh_total_bcase.coords,'\\n\\n')\n",
    "\n",
    "print('Here is the time coordinate for this DataArray:\\n')\n",
    "print(aice_nh_total_bcase['time'],'\\n\\n')\n",
    "\n",
    "print('This is the ice area time series as a numpy array:\\n')\n",
    "print(aice_nh_total_bcase.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your understanding of how to manipulate DataArrays, try printing the time values as a numpy array in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print time values as a numpy array\n",
    "\n",
    "print(aice_nh_total_bcase['time'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting NH sea ice area time series and seasonal cycle\n",
    "\n",
    "Enough figuring out tools. Let's do some science. <br> \n",
    "\n",
    "First, let's make a plot of the time series from the 4 simulations that we've loaded. Plotting the data at every step will ensure the we know what's going on and minimize mistakes. <br>\n",
    "\n",
    "For plotting, we will use the standard plotting library matplotlib. It is completely analogous to Matlab's plotting scripts, but uses Python syntax. If you are an R user, ggplot is a Python library based on R's ggplot2. If you prefer NCL, check out the Python library called PyNGL.  <br>\n",
    "\n",
    "To better see what each time series is doing, you can comment out the other plt.plot() calls using #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make some quicky plots\n",
    "\n",
    "#create some time arrays that make more sense \n",
    "#the \"time\" variable in the xarrays is \"days since\", but each starts at a different day\n",
    "time_b=aice_nh_total_bcase['time']\n",
    "time_e=aice_nh_total_ecase['time']\n",
    "time_f=aice_nh_total_fcase['time']\n",
    "time_g=aice_nh_total_gcase['time']\n",
    "\n",
    "f=plt.figure(figsize=(12,4))\n",
    "plt.plot(time_b/365,aice_nh_total_bcase/m2_to_millionkm2,label='b-case')\n",
    "plt.plot(time_e/365,aice_nh_total_ecase/m2_to_millionkm2,label='e-case') \n",
    "plt.plot(time_f/365,aice_nh_total_fcase/m2_to_millionkm2,label='f-case')\n",
    "plt.plot(time_g/365,aice_nh_total_gcase/m2_to_millionkm2,label='g-case')\n",
    "plt.xlabel('years since start')\n",
    "plt.ylabel('NH ice area (million km$^{2}$)')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zoomed in view of 300 years:\n",
    "\n",
    "#Comment out different lines to see individual lines better\n",
    "\n",
    "\n",
    "f=plt.figure(figsize=(12,4))\n",
    "plt.plot(time_b/365,aice_nh_total_bcase/m2_to_millionkm2,label='b-case')\n",
    "plt.plot(time_e/365,aice_nh_total_ecase/m2_to_millionkm2,label='e-case') \n",
    "plt.plot(time_f/365,aice_nh_total_fcase/m2_to_millionkm2,label='f-case')\n",
    "plt.plot(time_g/365,aice_nh_total_gcase/m2_to_millionkm2,label='g-case')\n",
    "plt.xlabel('years since start')\n",
    "plt.ylabel('NH ice area (million km$^{2}$)')\n",
    "plt.xlim([0,600])\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group discussion questions:\n",
    "\n",
    "Why does the F-compset case look like a big rectangular block, while the other cases have more variability? <br>\n",
    "\n",
    "Why does the maximum ice area of the G-compset case have obvious multidecadal variability? <br>\n",
    "\n",
    " - Hint: check out the CORE forcing protocol: https://www.sciencedirect.com/journal/ocean-modelling/special-issue/10PSR6J3BV4 <br>\n",
    " \n",
    "After figuring out what the CORE IAF protocol is, what makes the G-case here different from the B and E compset simulations? Is the comparison of these G simulations to these B/E simulations a clean comparison?\n",
    "\n",
    "Before you execute the next block, think about what differences you would expect in the seasonal cycle of these four cases, especially for the F-compset case. Write down your hypothesis before moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create monthly means\n",
    "#Side note: Here's a cool Python thing: The for loop is embedded in the array using Python \"list comprehension,\" \n",
    "#which is a concise way to create lists (a standard Python object that is made with square brackets []). \n",
    "#These lists are then turned into numpy arrays (using np.array()), though we didn't strictly need to do that\n",
    "bcase_monthlymean=np.array([np.mean(aice_nh_total_bcase[mm::12]) for mm in range(12)])\n",
    "ecase_monthlymean=np.array([np.mean(aice_nh_total_ecase[mm::12]) for mm in range(12)])\n",
    "fcase_monthlymean=np.array([np.mean(aice_nh_total_fcase[mm::12]) for mm in range(12)])\n",
    "gcase_monthlymean=np.array([np.mean(aice_nh_total_gcase[mm::12]) for mm in range(12)])\n",
    "\n",
    "#create a list with month abbrevations\n",
    "months_str=['J','F','M','A','M','J','J','A','S','O','N','D']\n",
    "\n",
    "#plot plot plot\n",
    "plt.plot(bcase_monthlymean/m2_to_millionkm2,label='b-case')\n",
    "plt.plot(ecase_monthlymean/m2_to_millionkm2,label='e-case') \n",
    "plt.plot(fcase_monthlymean/m2_to_millionkm2,label='f-case')\n",
    "plt.plot(gcase_monthlymean/m2_to_millionkm2,label='g-case')\n",
    "plt.xticks(range(12),months_str) #put month abbreviations as ticks on x-axis\n",
    "plt.ylabel('NH ice area (million km$^{2}$)')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group Discussion Question\n",
    "\n",
    "Which simulation shares the same seasonal cycle as the F-compset, and why? <br>\n",
    "- Hint: comment out each case to see. e.g. (#plt.plot(...))\n",
    "\n",
    "Any ideas why the seasonal cycle of the E (SOM) case has a smaller range than of the B and F cases? <br>\n",
    "\n",
    "Thoughts for what's up with the G case? <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the seasonal cycle\n",
    "\n",
    "Let's next take out the seasonal cycle and examine the anomalies in the NH sea ice area time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty arrays to fill\n",
    "deseason_b=np.empty(len(aice_nh_total_bcase))\n",
    "deseason_e=np.empty(len(aice_nh_total_ecase))\n",
    "deseason_f=np.empty(len(aice_nh_total_fcase))\n",
    "deseason_g=np.empty(len(aice_nh_total_gcase))\n",
    "\n",
    "#loop through each month and remove the monthly mean\n",
    "for mm in range(12):\n",
    "    deseason_b[mm::12]=aice_nh_total_bcase[mm::12]-bcase_monthlymean[mm]\n",
    "    deseason_e[mm::12]=aice_nh_total_ecase[mm::12]-ecase_monthlymean[mm]\n",
    "    deseason_f[mm::12]=aice_nh_total_fcase[mm::12]-fcase_monthlymean[mm]\n",
    "    deseason_g[mm::12]=aice_nh_total_gcase[mm::12]-gcase_monthlymean[mm]\n",
    "    \n",
    "#Side note: This is not the cleanest way to remove the monthly means. \n",
    "#There are really simple ways to remove the seasonal cycle with xarray methods\n",
    "#but they currently do not work for time series that start with Year 0 (ask Elizabeth if you\n",
    "#really want to learn exactly why it doesn't work). This is a current area of xarray development \n",
    "#that should be fixed very soon thanks to code development associated with the \"Pangeo\" project! :)\n",
    "    \n",
    "#make the figure\n",
    "#comment out individual plt.plot() calls to better see each time series\n",
    "f=plt.figure(figsize=(20,4))\n",
    "plt.plot(time_b/365,deseason_b/m2_to_millionkm2,label='b-case')\n",
    "plt.plot(time_e/365,deseason_e/m2_to_millionkm2,label='e-case') \n",
    "plt.plot(time_f/365,deseason_f/m2_to_millionkm2,label='f-case')\n",
    "plt.plot(time_g/365,deseason_g/m2_to_millionkm2,label='g-case')\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('NH sea ice area anomalies (million km$^{2}$)')\n",
    "\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's zoom in for a closer look at a couple of hundred years:\n",
    "\n",
    "f=plt.figure(figsize=(20,4))\n",
    "plt.plot(time_b/365,deseason_b/m2_to_millionkm2,label='b-case')\n",
    "plt.plot(time_e/365,deseason_e/m2_to_millionkm2,label='e-case') \n",
    "plt.plot(time_f/365,deseason_f/m2_to_millionkm2,label='f-case')\n",
    "plt.plot(time_g/365,deseason_g/m2_to_millionkm2,label='g-case')\n",
    "plt.xlim([0,600])\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('NH sea ice area anomalies (million km$^{2}$)')\n",
    "\n",
    "\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Group discussion questions\n",
    "Again, what's up with the F case? Why are the anomalies all 0? \n",
    "\n",
    "Why does the G case have the same repeating signal every 60-ish years?<br>\n",
    "\n",
    "- Hint: check out the CORE forcing protocol: https://www.sciencedirect.com/journal/ocean-modelling/special-issue/10PSR6J3BV4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple measures of variability\n",
    "\n",
    "Next, we're going to think about the variability in sea ice area in these four simulations. The first way we're going to do this is by simply taking the standard deviation of all these monthly time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare variability of monthly mean total Arctic sea ice area\n",
    "\n",
    "std_b_aice=aice_nh_total_bcase.std('time')/m2_to_millionkm2\n",
    "std_e_aice=aice_nh_total_ecase.std('time')/m2_to_millionkm2\n",
    "std_f_aice=aice_nh_total_fcase.std('time')/m2_to_millionkm2\n",
    "std_g_aice=aice_nh_total_gcase.std('time')/m2_to_millionkm2\n",
    "print('RAW OUTPUT STANDARD DEVIATIONS')\n",
    "print('Standard deviation of Arctic Ice Area in B:\\n',std_b_aice.values,'million sq km')\n",
    "print('Standard deviation of Arctic Ice Area in E:\\n',std_e_aice.values,'million sq km')\n",
    "print('Standard deviation of Arctic Ice Area in F:\\n',std_f_aice.values,'million sq km')\n",
    "print('Standard deviation of Arctic Ice Area in G:\\n',std_g_aice.values,'million sq km')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare variability of DESEASONALIZED monthly mean total Arctic sea ice area\n",
    "\n",
    "std_b_aice=np.std(deseason_b/m2_to_millionkm2)\n",
    "std_e_aice=np.std(deseason_e/m2_to_millionkm2)\n",
    "std_f_aice=np.std(deseason_f/m2_to_millionkm2)\n",
    "std_g_aice=np.std(deseason_g/m2_to_millionkm2)\n",
    "print('DESEASONALIZED STANDARD DEVIATIONS')\n",
    "print('Standard deviation of Arctic Ice Area in B:',std_b_aice,'million sq km')\n",
    "print('Standard deviation of Arctic Ice Area in E:',std_e_aice,'million sq km')\n",
    "print('Standard deviation of Arctic Ice Area in F:',std_f_aice,'million sq km')\n",
    "print('Standard deviation of Arctic Ice Area in G:',std_g_aice,'million sq km')\n",
    "\n",
    "\n",
    "#Side Note: We used two different methods to compute standard deviation in this cell and the one above.\n",
    "#Because the original output is in the \"DataArray\" format, we can take advantage of slick xarray methods\n",
    "#to do simple math (e.g., .std('time') which tells the object which dimension to do the standard deviation along). \n",
    "#When we deseasonalized the output, we put it into a numpy array, which doesn't have the fancy dimension tracking\n",
    "#like DataArray does (this is really helpful with 3+D arrays!). So, instead we use the numpy function np.std(STUFF).  \n",
    "#If we had put the deseasonalized object back into a DataArray (which would've required a couple more messy lines), \n",
    "#we could've used the xarray .std() method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group discussion questions\n",
    "\n",
    "Think about the differences in the standard deviations of these four compsets in the original versus the de-seasonalized data.  <br> <br>\n",
    "Why are some standard deviations less or more than the others? What do the standard deviations from the original output versus the deseasonalized output tell us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NH sea ice area in frequency space\n",
    "\n",
    "Now, we are going to look at the variability as a function of frequency. We are going to do a fast fourier transform of the four time series using the function \"periodogram\" from the scipy library. There are ~4-5 other ways to do FFT with other numpy and scipy functions. Periodogram is nice because it does a lot of the work for you (with normalization, etc.), requiring less coding from you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at this in spectral space\n",
    "\n",
    "#first remove the mean\n",
    "b_in=aice_nh_total_bcase-aice_nh_total_bcase.mean()\n",
    "e_in=aice_nh_total_ecase-aice_nh_total_ecase.mean()\n",
    "f_in=aice_nh_total_fcase-aice_nh_total_fcase.mean()\n",
    "g_in=aice_nh_total_gcase-aice_nh_total_gcase.mean()\n",
    "\n",
    "#calculate the frequency and spectral density, and then normalize\n",
    "#b-case\n",
    "f_b,Pxx_b = signal.periodogram(b_in,fs=1.,window='boxcar',nfft=None,return_onesided=True,scaling='spectrum')\n",
    "Pxx_bn = Pxx_b/np.sum(Pxx_b)\n",
    "\n",
    "#e-case\n",
    "f_e,Pxx_e = signal.periodogram(e_in,fs=1.,window='boxcar',nfft=None,return_onesided=True,scaling='spectrum')\n",
    "Pxx_en = Pxx_e/np.sum(Pxx_e)\n",
    "\n",
    "#f-case\n",
    "f_f,Pxx_f = signal.periodogram(f_in,fs=1.,window='boxcar',nfft=None,return_onesided=True,scaling='spectrum')\n",
    "Pxx_fn = Pxx_f/np.sum(Pxx_f)\n",
    "\n",
    "#g-case\n",
    "f_g,Pxx_g = signal.periodogram(g_in,fs=1.,window='boxcar',nfft=None,return_onesided=True,scaling='spectrum')\n",
    "Pxx_gn = Pxx_g/np.sum(Pxx_g)\n",
    "\n",
    "#plot it!\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "plt.plot(f_b*12,Pxx_bn,label='b-case')\n",
    "plt.plot(f_e*12,Pxx_en,label='e-case')\n",
    "plt.plot(f_f*12,Pxx_fn,label='f-case')\n",
    "plt.plot(f_g*12,Pxx_gn,label='g-case')\n",
    "plt.xlabel(\"frequency (year$^{-1}$)\");\n",
    "plt.ylabel('spectral density')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group discussion question\n",
    "\n",
    "Again, consider commenting out the four simulations to see what each is doing. <br>\n",
    "\n",
    "Why do these simulations show peaks at 1 and 2?  <br> <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's repeat, but with de-seasonalized data\n",
    "b_in_de=deseason_b-deseason_b.mean()\n",
    "e_in_de=deseason_e-deseason_e.mean()\n",
    "f_in_de=deseason_f-deseason_f.mean()\n",
    "g_in_de=deseason_g-deseason_g.mean()\n",
    "\n",
    "\n",
    "f_b,Pxx_b = signal.periodogram(b_in_de,fs=1.,window='boxcar',nfft=None,return_onesided=True,scaling='spectrum')\n",
    "Pxx_bn = Pxx_b/np.sum(Pxx_b)\n",
    "\n",
    "f_e,Pxx_e = signal.periodogram(e_in_de,fs=1.,window='boxcar',nfft=None,return_onesided=True,scaling='spectrum')\n",
    "Pxx_en = Pxx_e/np.sum(Pxx_e)\n",
    "\n",
    "f_f,Pxx_f = signal.periodogram(f_in_de,fs=1.,window='boxcar',nfft=None,return_onesided=True,scaling='spectrum')\n",
    "Pxx_fn = Pxx_f/np.sum(Pxx_f)\n",
    "\n",
    "f_g,Pxx_g = signal.periodogram(g_in_de,fs=1.,window='boxcar',nfft=None,return_onesided=True,scaling='spectrum')\n",
    "Pxx_gn = Pxx_g/np.sum(Pxx_g)\n",
    "\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "plt.plot(f_b*12,Pxx_bn,label='b-case')\n",
    "plt.plot(f_e*12,Pxx_en,label='e-case')\n",
    "plt.plot(f_f*12,Pxx_fn,label='f-case')\n",
    "plt.plot(f_g*12,Pxx_gn,label='g-case')\n",
    "plt.xlabel(\"frequency (year$^{-1}$)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we've effectively removed the seasonal cycle.  Now we see that the G-compset case has a lot of power at really low frequencies. Why? <br> <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's zoom in to low frequency to more clearly see differences\n",
    "#We've commented out the G-compset case because it's more interesting to compare the B/E compsets\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "gs=GridSpec(2,1)\n",
    "\n",
    "f=plt.subplot(gs[0,0])\n",
    "plt.plot(f_b*12,Pxx_bn,label='b-case')\n",
    "plt.plot(f_e*12,Pxx_en,label='e-case')\n",
    "plt.plot(f_f*12,Pxx_fn,label='f-case')\n",
    "#plt.plot(f_g*12,Pxx_gn,label='g-case')\n",
    "plt.xlabel(\"frequency (year$^{-1}$)\")\n",
    "plt.xlim([0,1.5])\n",
    "plt.legend()\n",
    "\n",
    "#plot same thing but in terms of 1/frequency, because that is a bit easier to grasp\n",
    "f=plt.subplot(gs[1,0])\n",
    "plt.plot(1/(f_b*12),Pxx_bn)\n",
    "plt.plot(1/(f_e*12),Pxx_en)\n",
    "plt.plot(1/(f_f*12),Pxx_fn)\n",
    "#plt.plot(1/(f_g*12),Pxx_gn)  #NOT PLOTTING G-CASE\n",
    "plt.xlabel(\"period (years)\")\n",
    "\n",
    "plt.xlim([1,100]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some differences in multidecadal peaks in the B and E compset cases.  Are these differences between the two datasets meaningful? And are these spectral peaks any different than what might be produced by noise? <br>\n",
    "\n",
    "First, let's test if any of the B and E compsets beat the null hypothesis that these time series are no different than red noise. <br>\n",
    "\n",
    "A red noise process is one in which random white noise is forcing the system and the system has some memory from timestep to timestep. If the processes included are not due to random noise (e.g., dynamics are involved), then the spectra will look different from that of a red noise process and have peaks that exceed a confidence interval constructed from a red noise spectra. <br>\n",
    "\n",
    "In the next few cells we will calculate the lag-1 autocorrelation for the B and E compset time series and produce red noise time series that uses this lag-1 autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A couple of functions for computing a red noise fit to a time series\n",
    "\n",
    "def create_normalized_redfit(data_length,Te):\n",
    "    freq = np.arange(0,(data_length/2)+1,1)/float(data_length) # to Nyquist\n",
    "    red_fit = (2 * Te)/(1 + ((2*np.pi*freq)**2)*(Te**2)) # After Hartmann 6.64, 6.91\n",
    "    return red_fit/np.sum(red_fit)\n",
    "def create_f_bounds(alpha,dof,red_fit_n):\n",
    "    f_ratio = stats.f.ppf(alpha,dof,200) # Akin to MATLAB's finv command\n",
    "    return f_ratio*red_fit_n\n",
    "\n",
    "#functions from CU ATOC Objective Analysis exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the above functions to calculate the red fit for the b and e cases\n",
    "\n",
    "## Calculate the power spectrum of red noise with lag1_r to use for significance testing\n",
    "alpha = 0.95 ## set statistical significance level\n",
    "\n",
    "#B-case first\n",
    "### step 1: calculate lag-1 autocorrelation (lag1_r, rho) and the associated p value (lag1_p)\n",
    "lag1_r,lag1_p = stats.pearsonr(b_in_de[0:len(b_in_de)-1],b_in_de[1:len(b_in_de)])\n",
    "### step 2: Calculate e-folding time for a red-noise process with this lag-1 autocorrelation\n",
    "Te = -1./np.log(lag1_r) # After Hartman 6.62 with delta t = 1\n",
    "print('B-case lag-1 autocorrelation =',round(lag1_r,2),'and e-folding time =',round(Te,0)) # rho = 0.911997, Te = 10.8555\n",
    "\n",
    "## calculate the power spectrum of red noise with lag1_r to use for significance testing\n",
    "bcase_red_fit = create_normalized_redfit(len(b_in_de),Te)\n",
    "dof_entirewindow=2 ### note dof=2 because using whole record for FFT with no chunking\n",
    "f_bounds_bcase = create_f_bounds(alpha,dof_entirewindow,bcase_red_fit)  ## using f-test for variance, see\n",
    "\n",
    "#E-case next\n",
    "### step 1: calculate lag-1 autocorrelation (lag1_r, rho) and the associated p value (lag1_p)\n",
    "lag1_r,lag1_p = stats.pearsonr(e_in_de[0:len(e_in_de)-1],e_in_de[1:len(e_in_de)])\n",
    "### step 2: Calculate e-folding time for a red-noise process with this lag-1 autocorrelation\n",
    "Te = -1./np.log(lag1_r) # After Hartman 6.62 with delta t = 1\n",
    "print('E-case lag-1 autocorrelation =',round(lag1_r,2),'and e-folding time =',round(Te,0)) # rho = 0.911997, Te = 10.8555\n",
    "\n",
    "## calculate the power spectrum of red noise with lag1_r to use for significance testing\n",
    "ecase_red_fit = create_normalized_redfit(len(e_in_de),Te)\n",
    "dof_entirewindow=2 ### note dof=2 because using whole record for FFT with no chunking\n",
    "f_bounds_ecase = create_f_bounds(alpha,dof_entirewindow,ecase_red_fit)  ## using f-test for variance, see\n",
    "\n",
    "#AND plot just B and E again\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "gs=GridSpec(2,1)\n",
    "\n",
    "colors=plt.cm.get_cmap('Paired')\n",
    "print(colors)\n",
    "\n",
    "f=plt.subplot(gs[0,0])\n",
    "plt.plot(f_b*12,Pxx_bn,label='b-case',color=colors(1/12))\n",
    "plt.plot(f_e*12,Pxx_en,label='e-case',color=colors(7/12))\n",
    "plt.plot(f_b*12,bcase_red_fit,label='red noise fit for b-case',linewidth=3,color=colors(0/12))\n",
    "plt.plot(f_b*12,f_bounds_bcase,label='95% bounds for b-case',linestyle='dashed',color=colors(0/12))\n",
    "plt.plot(f_e*12,ecase_red_fit,label='red noise fit for e-case',linewidth=3,color=colors(6/12))\n",
    "plt.plot(f_e*12,f_bounds_ecase,label='95% bounds for e-case',linestyle='dashed',color=colors(6/12))\n",
    "\n",
    "plt.xlabel(\"frequency (year$^{-1}$)\")\n",
    "plt.xlim([0,1.5])\n",
    "plt.legend()\n",
    "\n",
    "#plot the same thing but in terms of return period (rather than frequency)\n",
    "f=plt.subplot(gs[1,0])\n",
    "plt.plot(1/(f_b*12),Pxx_bn,color=colors(1/12))\n",
    "plt.plot(1/(f_e*12),Pxx_en,color=colors(7/12))\n",
    "plt.plot(1/(f_b*12),bcase_red_fit,label='red noise fit for b-case',linewidth=3,color=colors(0/12))\n",
    "plt.plot(1/(f_b*12),f_bounds_bcase,label='95% bounds for b-case',linestyle='dashed',color=colors(0/12))\n",
    "plt.plot(1/(f_e*12),ecase_red_fit,label='red noise fit for e-case',linewidth=3,color=colors(6/12))\n",
    "plt.plot(1/(f_e*12),f_bounds_ecase,label='95% bounds for e-case',linestyle='dashed',color=colors(6/12))\n",
    " \n",
    "    \n",
    "plt.xlabel(\"period (years)\")\n",
    "\n",
    "plt.xlim([1,100]);\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group discussion questions\n",
    "\n",
    "Where are the fully-coupled and slab simulations definitely exhibiting different behavior than that from a red-noise process?  (e.g., where do the spectral estimates exceed the 95% signficance bound?) <br>\n",
    "\n",
    "This doesn't tell us whether the B-compset peak and E-compset peak at 55-60 years are different from each other with 95% confidence.  <br>\n",
    "\n",
    "What can you conclude about the influence of the ocean at different time scales?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A couple of final things to think about\n",
    "\n",
    "If you wanted to figure out what is causing the big peaks at 60 years, what would you start to analyze? <br>\n",
    "\n",
    "Can you think of a science question related to your research where coupling might matter? <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPENDIX \n",
    "### In case you want to see the messy way in which the sea ice area from the original model output were processed:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "#loading in aice files\n",
    "\n",
    "#directory on glade for Large Ensemble\n",
    "lens_dir='/glade/p/cesm0005/CESM-CAM5-BGC-LE/ice/proc/tseries/monthly/aice/'\n",
    "\n",
    "#create list of files for b-case, e-case, f-case \n",
    "bfiles=sorted(glob.glob(lens_dir+'b.e11.B1850C5CN.f09_g16.005.cice.h.aice_nh.*.nc'))\n",
    "efiles=sorted(glob.glob(lens_dir+'e.e11.E1850C5CN.f09_g16.001.cice.h.aice_nh.*.nc'))\n",
    "\n",
    "lens_dir_fcase='/glade/p/cesm0005/CESM-CAM5-BGC-LE/atm/proc/tseries/monthly/ICEFRAC/'\n",
    "ffiles=sorted(glob.glob(lens_dir_fcase+'f.e11.F1850C5CN.f09_f09.001.cam.h0.ICEFRAC.*.nc'))\n",
    "#remember, f-case (AMIP) has specified sea ice\n",
    "\n",
    "#create list of files for g-case\n",
    "gcase_dir='/glade/p/decpred/CESM-DPLE_POPCICEhindcast/'\n",
    "gfiles=sorted(glob.glob(gcase_dir+'/g.e11_LENS.GECOIAF.T62_g16.009.cice.h.aice_nh.024901-031612.nc'))\n",
    "\n",
    "pickled_yet=0\n",
    "\n",
    "#load in the b-case NH ice area, then batch process down to a single time series and save as a python pickle\n",
    "\n",
    "if pickled_yet==0:\n",
    "    ds=xr.open_mfdataset(bfiles,decode_times=False)\n",
    "    areat=ds['tarea'].isel(time=0) #points to tracer grid area variable and puts it into a DataArray object \n",
    "    #areat is constant in time, only need one time, so only selecting one - saves memory later for computation\n",
    "    aice_nh=ds['aice'] #points to NH Ice area variable and puts into a DataArray object\n",
    "    print('pointed to bcase')\n",
    "    aice_nh_total_bcase=(aice_nh/100*areat).sum('nj').sum('ni')  #multiplies ice area fraction(ranges from 0 to 1) by area of grid cell\n",
    "    print('summed')\n",
    "    aice_nh_total_bcase.load() #pulls the relevant parts into memory\n",
    "    print('loaded b-case to memory')\n",
    "    #write out the timeseries as a pickle - this will save time for students\n",
    "    pickle.dump(aice_nh_total_bcase, open( \"aice_nh_total_bcase.p\", \"wb\" ))\n",
    "    print('written b-case to pickle')\n",
    "    ds.close() #close dataset to (hopefully) release the memory\n",
    "    \n",
    "    #repeat for e case\n",
    "    ds=xr.open_mfdataset(efiles,decode_times=False)\n",
    "    areat=ds['tarea'].isel(time=0)\n",
    "    aice_nh=ds['aice']\n",
    "    print('pointed to ecase')\n",
    "    aice_nh_total_ecase=(aice_nh/100*areat).sum('nj').sum('ni') \n",
    "    print('summed')\n",
    "    aice_nh_total_ecase.load()\n",
    "    print('loaded e-case to memory')\n",
    "    #write out the timeseries as a pickle - this will save time for students\n",
    "    pickle.dump(aice_nh_total_ecase, open( \"aice_nh_total_ecase.p\", \"wb\" ))\n",
    "    print('written e-case to pickle')\n",
    "    ds.close()\n",
    "    \n",
    "    #repeat for f case\n",
    "    ds=xr.open_mfdataset(ffiles,decode_times=False)\n",
    "    #areat does not exist for AMIP f-files so going to calculate total area by hand, sigh\n",
    "    lon_atm=ds['lon']\n",
    "    lat_atm=ds['lat']\n",
    "    #only want NH bits\n",
    "    half=int(len(lat_atm)/2)\n",
    "    lat_atm_half=lat_atm.isel(lat=slice(half,len(lat_atm)))\n",
    "\n",
    "    #assumes (correctly) that grid is evenly spaced\n",
    "    a=6370e3\n",
    "    dx=(lon_atm[3]-lon_atm[2]).values*np.pi/180*a*np.cos(lat_atm_half*np.pi/180)\n",
    "    dy=(lat_atm[3]-lat_atm[2])*np.pi/180*a\n",
    "    area_atmgrid=dx*dy\n",
    "\n",
    "    aice_nh=ds['ICEFRAC'].isel(lat=slice(half,len(lat_atm)))\n",
    "    print('pointed to fcase')\n",
    "    aice_nh_total_fcase=(aice_nh*area_atmgrid).sum('lon').sum('lat') \n",
    "    print('summed')\n",
    "    aice_nh_total_fcase.load()\n",
    "    print('loaded f-case to memory')\n",
    "    #write out the timeseries as a pickle - this will save time for students\n",
    "    pickle.dump(aice_nh_total_fcase, open( \"aice_nh_total_fcase.p\", \"wb\" ))\n",
    "    print('written f-case to pickle')\n",
    "    ds.close()\n",
    "    \n",
    "    #repeat for g case\n",
    "    ds=xr.open_mfdataset(gfiles,decode_times=False)\n",
    "    areat=ds['tarea']\n",
    "    aice_nh=ds['aice']\n",
    "    print('pointed to gcase')\n",
    "    aice_nh_total_gcase=(aice_nh/100*areat).sum('nj').sum('ni') \n",
    "    print('summed')\n",
    "    aice_nh_total_gcase.load()\n",
    "    print('loaded g-case to memory')\n",
    "    #write out the timeseries as a pickle - this will save time for students\n",
    "    pickle.dump(aice_nh_total_gcase, open( \"aice_nh_total_gcase.p\", \"wb\" ))\n",
    "    print('written g-case to pickle')\n",
    "    ds.close()\n",
    "    \n",
    "    pickled_yet=1\n",
    "elif pickled_yet==1:\n",
    "    #load in pickles if already exist\n",
    "    aice_nh_total_bcase=pickle.load(open( \"aice_nh_total_bcase.p\", \"rb\" ))\n",
    "    aice_nh_total_ecase=pickle.load(open( \"aice_nh_total_ecase.p\", \"rb\" ))\n",
    "    aice_nh_total_fcase=pickle.load(open( \"aice_nh_total_fcase.p\", \"rb\" ))\n",
    "    aice_nh_total_gcase=pickle.load(open( \"aice_nh_total_gcase.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
